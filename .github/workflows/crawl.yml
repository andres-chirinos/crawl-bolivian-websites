name: Crawl website

on:
  #schedule:
  #  - cron: "*/15 * * * *"
  workflow_dispatch:
    inputs:
      url:
        description: "URL to crawl"
        required: true
        default: ""
      depth:
        description: "Crawl depth [>=1]"
        required: false
        default: "10"
      scoreType:
        description: "Crawl score type [page, page-spa, {prefix}, host, domain, any, custom]"
        required: false
        default: "prefix"
      runs-on:
        description: "Runner type [{ubuntu-latest}, self-hosted]"
        required: false
        default: "ubuntu-latest"

jobs:
  extract:
    name: Extract
    runs-on: ${{ github.event.inputs.runs-on }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Read Config
        run: |
          if [[ "${ACTIONS_STEP_DEBUG}" == "true" ]]; then set -x; fi
          python -c """
          import yaml, datetime
          import os

          # Config variables
          def flatten_config(prefix, config):
            for key, value in config.items():
              if isinstance(value, dict):
                flatten_config(f'{prefix}{key.upper()}_', value)
              else:
                print(f'{prefix}{key.upper()}={value}')

          with open('config.yml') as f:
            config = yaml.safe_load(f)

          flatten_config('', config)

          # Runtime variables
          timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')
          print(f'TIMESTAMP={timestamp}')
          # Generate a URL slug for artifact naming
          import re
          url = os.environ.get('URL', '')
          url_slug = re.sub(r'[^A-Za-z0-9_-]', '_', url.replace('https://', '').replace('http://', ''))
          print(f'URL_SLUG={url_slug}')
          """ >> $GITHUB_ENV
        env:
          URL: ${{ github.event.inputs.url }}
          DEPTH: ${{ github.event.inputs.depth }}
          SCORE_TYPE: ${{ github.event.inputs.scoreType }}

      - name: Prepare Data
        run: |
          if [[ "$ACTIONS_STEP_DEBUG" == "true" ]]; then set -x; fi
          mkdir -p ${{ github.workspace }}/data/
          echo "Preparing data directory..."

      - name: Run Crawler
        env:
          URL: ${{ github.event.inputs.url }}
          DEPTH: ${{ github.event.inputs.depth }}
          URL_SLUG: ${{ env.URL_SLUG }}
          SCORE_TYPE: ${{ github.event.inputs.scoreType }}
        run: |
          if [[ "$ACTIONS_STEP_DEBUG" == "true" ]]; then set -x; fi
          echo "URL=$URL"
          echo "DEPTH=$DEPTH"
          echo "URL_SLUG=$URL_SLUG"
          echo "SCORE_TYPE=$SCORE_TYPE"

          # Detect if runner is self-hosted
          if [[ "${{ github.event.inputs.runs-on }}" == "self-hosted" ]]; then
            # Use a shared volume path for self-hosted runners
            # DATA_VOLUME="/mnt/runner-data/crawls"
            # mkdir -p "$DATA_VOLUME"
            # VOLUME_ARG="-v $DATA_VOLUME:/crawls/"
            VOLUME_ARG="-v ${{ github.workspace }}/data:/crawls/"
          else
            # Default to workspace for GitHub-hosted runners
            VOLUME_ARG="-v ${{ github.workspace }}/data:/crawls/"
          fi

          docker run --rm $VOLUME_ARG webrecorder/browsertrix-crawler crawl \
            --url "$URL" \
            --generateWACZ \
            --blockads \
            --adBlockMessage "AD BLOCKED" \
            --blockMessage "URL BLOCKED" \
            --depth "$DEPTH" \
            --timeout 120 \
            --scoreType "$SCORE_TYPE" \
            --allowHashUrls \
            --useSitemap \
            --waitUntil networkidle0 \
            --lang es \
            --retries 3 \
            --postLoadDelay 1 \
            --collection "$URL_SLUG"-"$TIMESTAMP" \
            --workers 4 \
            --clickSelector "a,button" \
            --title "$URL_SLUG" \
            --description "$URL_SLUG - $TIMESTAMP"

      - name: Check artifact content
        run: |
          if [[ "${ACTIONS_STEP_DEBUG}" == "true" ]]; then set -x; fi
          echo "Checking ./data/ content before upload:"
          ls -l ${{ github.workspace }}/data/
          count=$(find ${{ github.workspace }}/data/ -type f | wc -l)
          find ${{ github.workspace }}/ -maxdepth 2 -print
          if [[ "$count" -eq 0 ]]; then
            echo "ERROR: No files found in ./data/ to upload as artifact."
            exit 1
          fi

      - name: Upload raw data
        id: upload-raw-data-step
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.URL_SLUG }}-${{ env.TIMESTAMP }}
          path: ${{ github.workspace }}/data/*
          retention-days: ${{ env.GITHUB_ACTIONS_ARTIFACT_DATA_RETENTION_DAYS }}
          if-no-files-found: error

  load:
    name: Load
    needs: extract
    runs-on: ubuntu-latest
    steps:
      - name: Download raw data
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.URL_SLUG }}-${{ env.TIMESTAMP }}.zip
          path: ./data/

      - name: Zip raw data
        run: |
          if [[ "${ACTIONS_STEP_DEBUG}" == "true" ]]; then set -x; fi
          zip -r ${{ env.URL_SLUG }}-${{ env.TIMESTAMP }}.zip ${{ github.workspace }}/data/

      - name: Upload to Google Drive
        uses: willo32/google-drive-upload-action@v1
        with:
          target: ${{ env.URL_SLUG }}-${{ env.TIMESTAMP }}.zip
          credentials: ${{ secrets.GDRIVE_SERVICE_ACCOUNT }}
          parent_folder_id: ${{ env.DRIVE_FOLDER_ID }}
          name: ${{ env.URL_SLUG }}-${{ env.TIMESTAMP }}.zip
          child_folder: ${{ env.URL_SLUG }}
