name: Crawl website

on:
  #schedule:
  #  - cron: "*/15 * * * *"
  workflow_dispatch:
    inputs:
      url:
        description: "URL to crawl"
        required: true
        default: ""
      depth:
        description: "Crawl depth [>=1]"
        required: false
        default: "10"
      scoreType:
        description: "Crawl score type [page, page-spa, {prefix}, host, domain, any, custom]"
        required: false
        default: "prefix"
      runs-on:
        description: "Runner type [{ubuntu-latest}, self-hosted]"
        required: false
        default: "ubuntu-latest"

jobs:
  extract:
    name: Extract
    runs-on: ${{ github.event.inputs.runs-on }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Read Config
        run: |
          python -c """
          import yaml, datetime
          import os

          # Config variables
          def flatten_config(prefix, config):
            for key, value in config.items():
              if isinstance(value, dict):
                flatten_config(f'{prefix}{key.upper()}_', value)
              else:
                print(f'{prefix}{key.upper()}={value}')

          with open('config.yml') as f:
            config = yaml.safe_load(f)

          flatten_config('', config)

          # Runtime variables
          timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')
          print(f'TIMESTAMP={timestamp}')
          # Generate a URL slug for artifact naming
          import re
          url = os.environ.get('URL', '')
          url_slug = re.sub(r'[^A-Za-z0-9_-]', '_', url.replace('https://', '').replace('http://', ''))
          print(f'URL_SLUG={url_slug}')
          """ >> $GITHUB_ENV
        env:
          URL: ${{ github.event.inputs.url }}
          DEPTH: ${{ github.event.inputs.depth }}
          SCORE_TYPE: ${{ github.event.inputs.scoreType }}

      - name: Run Crawler
        env:
          URL: ${{ github.event.inputs.url }}
          DEPTH: ${{ github.event.inputs.depth }}
          URL_SLUG: ${{ env.URL_SLUG }}
          SCORE_TYPE: ${{ github.event.inputs.scoreType }}
        run: |
          docker run --rm -v ${{ github.workspace }}/data:/crawls/ webrecorder/browsertrix-crawler crawl --url "$URL" --generateWACZ --blockads --adBlockMessage "AD BLOCKED" --blockMessage "URL BLOCKED" --depth "$DEPTH" --timeout 120 --scoreType "$SCORE_TYPE" --allowHashUrls --useSitemap --waitUntil networkidle0 --lang es --retries 3 --postLoadDelay 1 --collection "$URL_SLUG"-"$TIMESTAMP" --workers 4 --clickSelector "a,button" \ --screenshot ["view", "thumbnail", "fullPage", "fullPageFinal"] --title "$URL_SLUG" --description "$URL_SLUG - $TIMESTAMP"

      - name: tree
        run: |
          tree ${{ github.workspace }}/

      - name: Upload raw data
        id: upload-raw-data-step
        uses: actions/upload-artifact@v4
        with:
          name: raw-data-${{ env.URL_SLUG }}-${{ env.TIMESTAMP }}
          path: ${{ github.workspace }}/data/*
          retention-days: ${{ env.GITHUB_ACTIONS_ARTIFACT_DATA_RETENTION_DAYS }}

      - name: Zip raw data
        run: |
          zip -r raw-data-${{ env.URL_SLUG }}-${{ env.TIMESTAMP }}.zip ${{ github.workspace }}/data/

      - name: Upload to Google Drive
        uses: willo32/google-drive-upload-action@v1
        with:
          target: raw-data-${{ env.URL_SLUG }}-${{ env.TIMESTAMP }}.zip
          credentials: ${{ secrets.GDRIVE_SERVICE_ACCOUNT }}
          parent_folder_id: ${{ env.DRIVE_FOLDER_ID }}
          name: ${{ env.URL_SLUG }}-${{ env.TIMESTAMP }}.zip
          child_folder: ${{ env.URL_SLUG }}
